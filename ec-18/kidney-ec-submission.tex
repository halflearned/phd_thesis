\documentclass[format=acmsmall, review=false]{acmart}

\usepackage{acm-ec-18}

\usepackage{booktabs} % For formal tables

\usepackage[ruled]{algorithm2e} % For algorithms

\usepackage{ textcomp }

\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\newcommand\mycommfont[1]{\footnotesize\upshape\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\begin{document}
% Title portion. Note the short title for running heads 
\title[New strategies for dynamic kidney exchange]{New strategies for dynamic kidney exchange}  
\author{Submission 42}

% note that the abstract must come before \maketitle
\begin{abstract}
[[Kidney exchange is great, provides a way to increase number of lives saved]] [[Most algorithms static, don't think about future]] [[There might be ways to improve, but literature still small]] [[This work adds two contributions to the literature]] [[Direct prediction method, MAB-based methods]] [[The latter can improve the average number of kidneys exchange by 3\% over the myopic algorithm]]
\end{abstract}

% note: this command has been disabled to remove the ACM copyright block. Sorry...
%\thanks{This work is supported by the National Science Foundation,
%  under grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.}

\maketitle

\section{Introduction}

[[Kidney exchange]]

[[Will interchangeably use the words 'node', 'vertex' and 'pair']]

\paragraph{\textbf{Main idea}} We propose and empirically evaluate two novel approaches for increasing the cardinality of matched pairs in a discrete time dynamic model of kidney exchange. Our two methods are named the \emph{direction estimation} and \emph{multi-armed-bandit} methods. The former recasts the dynamic kidney exchange problem as a binary classification task: for each pair in the pool, we predict a binary label representing whether they should be matched today (one), or left for the future (zero), given their observable characteristics. The latter uses simulations to select exchanges that are less likely to decrease the size of an optimal future matching. We restrict our analyses to pairwise exchanges (i.e., only two-cycles are allowed), but the ideas introduced here can be easily extended to more complex exchanges, computational constraints permitting.

Each of our two approaches faces their own challenges. In the \emph{direct estimation} method, one challenge is that an important part of our data -- the compatibility graph -- lives in a non-Euclidean space, and it is not obvious how to use it as an input to a classification algorithm. We attempt to apply a variety of new methods from the graphical signal processing literature to tackle this problem. [[[An additional challenge is that traditional classification algorithms are not equipped to produce dependent labels, but in our setting whether or not we decide that a certain pair is to be matched today ought to be related to our decisions about everyone else in the pool.]]]] 

In the \emph{multi-armed bandit} method, we would like to predict how likely it is that a certain pair should be reserved for a future exchange, and match those less frequently. However, computationally costly simulations and the enormous branching factor of our problem precludes us from accurately estimating of this probability. In order to overcome this challenge, we employ multi-armed bandit (MAB) algorithms that optimally choose which exchange to collect statistics about under a relatively small computational budget.


\paragraph{\textbf{Main results}} Our methods are evaluated via simulations. The simulation setups, which we call \emph{environments}, are inspired by models used the kidney exchange literature, and differ by their rules regarding blood- and tissue-type compatibility.

We apply our methods to each simulation setup and compare them to a \textsc{myopic} algorithm that clears the maximal matching at each period, and to an infeasible \textsc{OPT} algorithm that is able to observe future states of the world and act optimally. Our metric for comparison is the per-period average number of matched pairs over a long period of time.

In all environments, we observe that the \emph{direct estimation} unfortunately does equally well or worse than the myopic algorithm. However, the \emph{multi-armed bandit} method is able to improve substantially upon the \textsc{Myopic} benchmark, [[[suggesting that we could save more lives etc. ]]]]

\subsection{Related literature} This paper pulls ideas from microeconomics and matching theory, and uses methods drawn from computer science, machine learning, and operations research. Let us take a brief look at these fields in turn. 

\paragraph{Matching theory} Kidney exchange as an economic problem was first studied in a series of papers by \citet{roth2004kidney, roth2005pairwise, roth2007efficient}, but the fist paper on dynamic kidney exchange was written a few years later by \citet{unver2010dynamic}, who derived an optimal matching mechanism in a simple continuous time model. Two of the simulation setups in our work follows a modified, discrete-time version of the models in these seminal papers.

In the computer science literature, \citet{awasthi2009online} and later \citet{dickerson2015futurematch} wrote computational alternatives to simple myopic kidney exchange. Our methods are similar to the \emph{weighted myopia} scheme seen in these papers. Other papers that influence ours are \citet{akbarpour2017thickness} and \citet{ashlagi2013kidney}.



\paragraph{Computer science and machine learning}



%%%%%%%%%
\section{Simulation environments}

\paragraph{Basic definitions and notation} 

An \emph{agent} is an algorithm that decides who should be matched. 


\subsection{Common features across environments}

\paragraph{Poisson entry, Geometric death} At each period, the number of new incoming pairs is drawn from the $Poisson(\lambda)$ distribution, where $\lambda \in \mathbb{N}$ denotes the \emph{entry rate}, and equals the expected number of entrants per period. Upon entrance, each pair independently draws the length of their sojourn from the $Geometric(d)$ distribution. The parameter $d \in \mathbb{R}$ is the \emph{death rate}, and its reciprocal $\frac{1}{d}$ is the expected sojourn length. We note that, due to the memoryless property\footnote{If $X \sim Geometric(p)$, then $P(X > t+s | X > s) = P(X > t)$} of the Geometric distribution, the amount of time a pair has waited in the pool gives us no information about how much time they have until their death.

\paragraph{Binary preferences} In this work we abstract from these concerns and consider every exchange to be equally desirable, so long as it is available. This is the same assumption as used in \cite{roth2005pairwise}. 

\subsection{ABO Environment}

In the \emph{ABO environment}, compatibility between two distinct pairs is based only on blood-type compatibility. Blood types are drawn independently for patient and donor from the US population (roughly $O$:$49\%$, $A$:$36\%$, $B$:$11\%$, $AB$:$4\%$). In addition, we make an assumption previously used in \citet{unver2010dynamic} to allow for incompatibility between a donor and their own patient. The reason for this additional assumption is that, if there were truly no tissue type compatibilities, we would never observe pairs of type $(AB,\cdot)$, $(\cdot, O)$, or $(A,A)$, $(O,O)$, $(B,B)$, and $(AB,AB)$, since their donors would be automatically compatible with their patients and they would never participate in an exchange. Following \citet{zenios2001primum}, we assume that for such patients the probability that a donor and their patient are incompatible is $p_c = 0.11$. Arrival rates are adjusted accordingly, e.g., the arrival rate of $(A,O)$ pair is proportional to $0.48 \times 0.36 \times 0.11 \approx 0.019$.

\subsection{RSU Environment}

The \emph{RSU} environment is named after the simulation model in \citet*{roth2007efficient}. Each pair is characterized by patient and donor ABO blood types, current waiting time, and a \emph{calculated panel reactive antibody (cPRA)} level that governs the probability of a crossmatch with a random donor.\footnote{The original \citet{roth2007efficient} paper called this simply \emph{PRA}. In real life there is an important distinction between the two measures, but for the purposes of a simulation model this distinction is immaterial. We keep the name \emph{cPRA} for consistency with OPTN environment later.} The lower the cPRA, the higher the number of potentially compatible pairs.

The simulation process is as follows. First, we draw a pair in the same manner as in the ABO environment. Next, we draw if the patient is a female (with probability around 41\%), and if so we also draw whether her donor is her husband (spouses comprise about 49\% of donors). Finally, we draw a  cPRA level for the patient (Low: 70.1\%, Medium: 20\%, High: 9.9\%). This cPRA level determines the probability that they can receive a kidney from any donor, including their own: patients with low cPRA have a 5\% probability of positive crossmatch with a random donor; patients with medium cPRA have a 45\% chance, and patients with high cPRA have a 90\% chance of a crossmatch. If a patient is bloody or tissue-type incompatible with their own donor, they enter the pool. In addition, if the patient is female and her husband is the donor, the probability of positive crossmatch for low, medium and high cPRA patients goes up to 28.75\%, 58.75\% and 92.25\%. This last adjustment reflects the fact that women tend to produce antibodies against their husbands' antigens during pregnancy.

Once in the pool, the pair immediately forms directed edges with the existing pairs, again following the patient cPRA distribution. The resulting random graph is akin to a Erd\"{o}s-R\'{e}nyi $G(n,p)$ random graph where the probability of forming edges is heterogeneous across different pair types.

\subsection{OPTN Environment}

In the \emph{OPTN environment}, we use historical data collected by the United Network for Organ Sharing (UNOS) data provided in the Standard Research and Analysis (STAR) dataset. The STAR dataset contains information from all patients that were ever registered to the kidney waiting list in the United States for the past three decades, as well as from living donors that participated in an transplant. From this original dataset, we excluded patients that were registered for more than one organ (including kidney+pancreas), patients that were not waiting for their first kidney transplant, and patients and donors with incomplete HLA profile information. The resulting dataset contained 117813 patients and 9337 living donors.

\paragraph{Patient and donor cPRA} We added two additional variables to the original dataset: a \emph{patient cPRA} and a \emph{donor cPRA}. While the patient cPRA is a measure of patient tissue-type incompatibility with a random donor \cite{cecka2010calculated}, our \meph{donor cPRA} is a novel measure of the opposite direction -- how frequently a donor is tissue-type incompatible with a random patient.\footnote{We thank Itai Ashlagi for the suggestion of a donor cPRA.} Both were computed empirically: for each patient our dataset, we checked the how many donors exhibited antigens that are unacceptable for the patient in any of the A, B, Bw, C, DR, DPB, DQ, and DQA loci, and assigned this positive crossmatch probability as their patient cPRA; for the donors, we worked in the opposite direction by calculating the frequency of patients who exhibited antibodies against their donor's antigens, and that became their donor cPRA. Figure \ref{fig:cpra} shows the distribution across the entire population.\footnote{We should remark that we found a very different patient cPRA distribution than the one in the OPTN dataset. This may have been because: in real life cPRA is computed using deceased donor data, while we used living donor data; we may have used different HLA equivalence tables; OPTN uses a more sophisticated model based on population genetics.\cite{optn2013cpra}}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/computed_cPRA.pdf}
\caption{\textbf{Patient and donor cPRA} \\
    Computed from historical data. Our patient cPRA is the frequency of living donors that exhibit antigens that are unacceptable to the patient. We also define a donor cPRA by calculating the frequency of patients that have antibodies against the donors antigens.}
\label{fig:cpra}
\end{figure}


\paragraph{Artificial dataset} Next, we created an artificial dataset by randomly drawing patients and living donors from the historical dataset and checking for blood-type compatibility, and tissue-type compatibility as explained above. Compatible pairs were discarded. We iterated in this manner to construct a dataset of about one million incompatible pairs. Its columns are a set of dummies representing patient and donor blood times, a set of dummies for each allele in each HLA locus, and 

At every simulation period, a random number of pairs are drawn from this dataset.

[[[[Algorhtm can still see original alleles. Patient and donor cPRA offer "long-run" measure of compatibility]]]]


%%%%%%%%
\section{Methods}

\subsection{Objective and benchmarks}

In this work, we focus on maximizing the average number of matched pairs over $T$ periods, where $T$ is a large number relative to the rates of patient entry and death. We are interested in how our new methods perform relative to the following two benchmarks.

\paragraph{\textsc{Myopic}}

This algorithm finds the maximal matchings at every period, and clears it immediately. It disregards all observable characteristics of each pair, and in particular it disregards that some pairs might be useful to keep certain pairs may be easier or harder to match. In doing so, it may forgo the opportunity of matching a hard-to-match patient today, or postpone an easy-to-match pair for later. In essence, this is an approximation to what kidney exchanges currently do.

\paragraph{Optimal (\textsc{OPT})} This infeasible algorithm (henceforth OPT) does away with the uncertainty arising from the temporal structure of the problem: it knows exactly which pairs will come into the pool, as well as their arrival, and the duration of their sojourns. This algorithm is essentially a large static kidney matching problem, except that the set of available exchanges has the additional constraint that a cycle cannot exist between two vertices if their sojourns fail to overlap.

\paragraph{\textbf{Remark}} \textit{How much is there to be improved upon?} In Figure \ref{fig:greedy_opt_comparison}, we compare \textsc{Myopic} and \textsc{OPT} for a grid of different entry and death rates. These results show that the performance gap between the two can be fairly large, in particular in sparser environments like \emph{RSU} and \emph{OPTN}. We also note that the gap is narrower when: the death rate is high, because if most pairs will die soon, dynamic considerations play a smaller role; and when the entry rate is very large, because in a thicker market pairs are able to encounter a suitable match more easily.

\begin{figure}[H]
\centering
\hspace*{-3.5cm}
\includegraphics[width=1.4\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/greedy_opt_comparison_mcl2_2.pdf}
\caption{\textbf{Comparing \textsc{Myopic} and \textsc{OPT}}  \\
Ratio of average per-period matched pairs for different entry and death rates, over 3000 periods (darker hues are better). \texttt{Myopic} has better chances of achieving performances similar to \texttt{OPT} when the death rate is high (moving rightwards on the graphs), or when entry rate is high (moving downwards on the graphs).}
\label{fig:greedy_opt_comparison}
\end{figure}


% ---------------------------------------------------------------------------
% Algorithms
% ----------------------------------------------------------------------


\section{Algorithms}

We now present two novel methods to determine which patients should be matched.

\subsection{Direct prediction} \label{subsec:direct_prediction}

Dynamic and static algorithms can work in tandem: the former can determine which pairs should be matched today, while the latter decides how they should be matched among themselves. The \emph{direct prediction}, exploits this insight, essentially reducing the problem to a classification task: at each period, we aim to produce a binary label for each node indicating whether is should be matched in this period (1) or left for later (0). Selected nodes are then passed to a static solver that finds the maximal matching among them. Once these nodes are cleared, time evolves to the next period. The procedure is formalized in Algorithm \ref{alg:direct_prediction}, and also illustrated in Figure \ref{fig:direct_prediction}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/direct_prediction.pdf}.
\caption{\textbf{Direct prediction method} \\ One period of simulation using direct prediction methods to choose cycles. See Algorithm \ref{alg:direct_prediction} for details.}
\label{fig:direct_prediction}
\end{figure}


\begin{algorithm}[htbp]
	\SetAlgoLined
  \DontPrintSemicolon
	
  \KwData{Environment simulator object \textsc{Env}; \\
          Integer programming solver object \textsc{Solver}; \\
          Statistical method \textsc{Classifier} \\
          Threshold $thres$; }

  \SetKwFunction{FChoose}{\textsc{direct\_prediction}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FChoose{\textsc{Env}, \ \textsc{Solver}, \ \textsc{Classifier}, \ $thres$}}{
    \tcp{Retrieve node (and potentially also graph) data from current environment} \\
    $X, E_1, E_2$ \gets \textsc{Env.get\_data}()  \\
    \tcp{Classifier predicts matching probability for each pair using data} \\
    $prob$ \gets \textsc{Classifier}($X$, \ $E_1$, $E_2$)  \\
    \tcp{Get index of pairs whose probability is higher than threshold} \\
    $index$ $\gets$ \textsc{which}($prob > thres$) \\
    \tcp{Find maximal matching restricted to this subset} \\
    chosen\_cycles $\gets$ \textsc{Solver.solve(Env, subset=$index$)}  \\  
    \tcp{Return chosen cycles to be cleared} \\
    \KwRet chosen\_cycles \;
  }
	\caption{Function \textsc{direct\_prediction}}
	\label{alg:direct_prediction}
\end{algorithm}


\subsection{Direct prediction methods}

In order to produce data to feed into our ``classifier", we repeatedly created 1000-period simulation runs and solved using them OPT using with each each node's observable characteristics as regressors, and a binary variable indicating whether they were matched or not as the regressand. The resulting artificial dataset of approximately 2 million observations was then fed to a series of predictive algorithms, including penalized logistic regression \citep{wu2009genome}, support vector machines with radial basis function kernels \citep{cortes1995support}, random forest classifiers \citep{breiman2001random}, gradient tree boosting classifiers \citep{friedman2001greedy} and deep feedforward networks \citep{goodfellow2016deep}. 

Moreover, in order to represent information about the graph and how the nodes fit within it, we augment the original dataset using one or both of the following. First, with vertex properties for each node such as the number of in and out-edges, average neighbor degree, some centrality measures (betweenness, in-degree, out-degree, harmonic, closeness) and more exotic indices such as the core number and pagerank of each node\cite{newman2010networks}. Second, we augment the data using the \emph{node2vec} graph embedding algorithm of \cite{grover2016node2vec}, a continuous feature representation algorithm inspired by the skip-gram model.[[[More on this?]]]. These algorithms will produce auxiliary matrices $E_t^{1}$ and $E_t^2$ that are concatenated to the original data regressor matrix $X_t$, and the entire augmented data set can again be fed to the predictive algorithms above. Their performance is compared on the bottom half of Table \ref{tab:direct_prediction}.


\subsection{Multi-armed bandit methods}

We have access to the data-generating process (\emph{environments}) and an algorithm that is able to find the best matching for any data in hindsight (\texttt{OPT}). Therefore, in principle we could estimate the best choice of cycle to clear by simulating several periods ahead in the future, running \texttt{OPT}, and measuring the performance of our choice under any desired criterion. 

The approach outlined above turns out to be naive and incomplete, but it essentially contains the insight under which we will be working in this section. It is naive because simulations are computationally expensive, and in practice we cannot repeat them enough times get reliable estimates of the average performance for each cycle choice, especially in large graphs. It is also incomplete because it does not specify exactly what is the best information we should extract from \texttt{OPT} results. In order to solve the first problem, we leverage theory and algorithms from the multi-armed bandit (MAB) literature. For the second, we propose a secondary objective based on what we call \emph{pseudo-rewards}.

Our procedure is illustrated in Figures \ref{fig:mab} and \ref{fig:mab_step}. At the beginning of the period $t$, the agent receives a set of cycles $C$ that are available to be cleared. If $C$ is empty, nothing happens and we move to the next period. Otherwise, the agent then picks a cycle $c \in C$ and simulates the future, including new entries and deaths, up to a horizon $h$. Next, \texttt{OPT} is run twice, once normally, and once with the additional constraint that $c$ be removed today. The size of the resulting matching in these two scenarios is compared. Naturally, the constrained version of \texttt{OPT} cannot achieve anything better than its unconstrained counterpart, but it might get to be equal. If it is, the agent receives a \emph{pseudo-reward} of one, otherwise it receives zero. This process is repeated: at each iteration $\ell$, a cycle $c_{\ell}$ is chosen and its pseudo-reward $r_{c, \ell}$ is revealed. When a preset computational budget of $L(|C|)$ iterations is hit, the agent then analyses the whole history of cycle choices and pseudo-rewards $H_{t} = \{ (c_{\ell}, r_{c\ell} ) \}_{\ell=1}^{L}$, and decides whether to match one of the cycles or move on to the next period. If a cycle $c$ is chosen it is immediately cleared, however the environment does not evolve to the next period yet. Instead, the history $H_t$ is discarded the procedure is repeated again with a reduced set of actions $C' \subset C$ that produces a new history $H_t'$ and so on, until either there are no more available choices or the agent decides to allow the environment to move on to time $t+1$. When that at last happens, entries and deaths are revealed, the agent receives a new set of cycles, and the process begin anew. All past information is ignored.


Two important details were left out of the explanation above. First, how does the agent chooses the next cycle to test? Second, how does it decide which cycle to choose (or no cycle at all)? 

Let $c^{*}$ be a cycle that maximizes expected pseudo-rewards during one round of the algorithm:
$$c_{\ell}^{*} \in \arg\max_{c} E[r_{c,\ell}] $$


 Also, let \emph{regret} be defined as the difference between expected reward of the optimal choice $c^*$ and its own selected choice $c_{\ell}$.
 
 $$\Delta_\ell := E[r_{c^{*},\ell}] - E[r_{c,\ell}]$$
 
 A \emph{multi-armed bandit} (MAB) algorithm is a procedure to minimize the cumulative regret over $L$ rounds. A good MAB algorithm will act so as to balance exploration (trying out different choices to get high-quality estimates of their rewards) and exploitation (using out better choices more often to increase total rewards). 
 
 The literature on bandit algorithms is extensive and spans at least seventy years of research \citep{lattimore2018bandits}. Here we will focus on binary 
 
 % Algorithm
 \begin{algorithm}[htbp]
 	\SetAlgoLined
 	\KwData{Cycle $c$; Horizon $h$; \\
           Lists pseudo-reward statistics $Avg$, $Std$; \\
           Environment simulator object \textsc{Env}; \\
           Oracle solver object \textsc{OPT}; }
   \DontPrintSemicolon
   \SetKwFunction{FChoose}{\textsc{get\_pseudo\_reward}}
   \SetKwProg{Fn}{Function}{:}{}
   \Fn{\FChoose{\textsc{Env}, \ $c$, \ $Avg$, \ $Std$, \ $h$ }}{
     \tcp{Simulate up to horizon $h$ and find optimal matching} \\
     \textsc{Env.simulate}($h$)  \\
     $r_1$ \gets \textsc{OPT.solve}(\textsc{Env})  \\
     \tcp{Remove cycle $c$, find constrained optimal matching} \\
     \textsc{Env.remove}($c$)  \\
     $r_2$ \gets \textsc{OPT.solve}(\textsc{Env}) \\
     \tcp{Return 1 if rewards are equal, 0 otherwise} \\
     \KwRet $r_1 == r_2$ \;
   }
 	\caption{Function \textsc{get\_pseudo\_reward}}
 	\label{alg:get_pseudo_reward}
 \end{algorithm}
 
 
 
 \begin{algorithm}[htbp]
 	
   \SetAlgoLined
   \DontPrintSemicolon
 	
   \KwData{Horizon $h$; Number of iterations $L$; Threshold $thres$; \\
     Environment simulator object \textsc{Env};
     Multi-armed bandit algorithm object \textsc{MAB};
   }
 
   \SetKwFunction{FChoose}{\texttt{choose\_cycle}}
   \SetKwProg{Fn}{Function}{:}{}
   
   \Fn{\FChoose{\texttt{Env},\ \texttt{h} }}{
     \tcp{Initialize lists of current pseudo-reward statistics} \\
     $C$ \gets \textsc{Env.get\_available\_cycles()} \\
     $Avg$ \gets \textsc{zeros(length(C))} \\
     $Std$ \gets \textsc{zeros(length(C))} \\
     
     \tcp{Begin iterations} \\
     \For{$i\gets0$ \KwTo $L$}{
       \tcp{Bandit algorithm chooses next cycle to test given statistics} \\
       $c$ \gets \textsc{MAB.pull($C$, Avg, Std)}  \\
       \tcp{Compute pseudo reward for this cycle and update statistics} \\
       $r$ \gets \textsc{get\_pseudo\_reward}(\textsc{Env},\ $c$,\ $Avg$,\ $Std$,\ $h$) \\
       $Avg$ \gets \textsc{update\_running\_average($Avg$, $r$)} \\
       $Std$ \gets \textsc{update\_running\_std($Std$, $r$)} \\ 
     }  
     \tcp{Bandit algorithm chooses best cycle given statistics} \\
     $c\_best$ \gets \textsc{MAB.choose($C$, Avg, Std)} \\
     \tcp{Return best cycle, unless none of the pseudo-reward averages are above a certain threshold} \\
     \eIf{\textsc{All}($Avg \leq thres$)} {
       \KwRet \text{NULL} \\ 
       }{
       \KwRet $c$ \\
       }
     }
   }
 	\caption{Function \texttt{choose\_cycle}}
 	\label{alg:choose_cycle}
 \end{algorithm}


 \begin{figure}[htbp]
  \centering
  %\hspace{-1cm}
  \includegraphics[width=1\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/mab.pdf}
  \caption{\textbf{Multi-armed bandit methods} \\ 
  One period of simulation using multi-armed bandit methods to choose cycles.}
  \label{fig:mab}
  \end{figure}

  \begin{figure}[htbp]
  \centering
  %\hspace*{-0.5cm}
  \includegraphics[width=1\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/mab_step.pdf}
  \caption{\textbf{Function} \textsc{get\_pseudo\_reward} \\ Multi-armed bandits evaluate if a cycle should be cleared by checking if there is a high chance that the cycle will be used in the future. Such cycles get a \emph{lower} reward, and are left for later.}
  \label{fig:get_pseudo_reward}
  \end{figure}

\paragraph{What are pseudo-rewards?} As we match and clear out a cycle $c$ today, we forgo the opportunity of using any future cycles involving the nodes in $c$. However, because there may be multiple optimal matchings, sometimes the cycles that become unavailable due to the removal of the nodes in $c$ do not matter, and we can still find a matching of the same cardinality without them. In other words, we pay no price for removing these future cycles.
 
The pseudo-reward associated with cycle $c$ is a random variable whose expectation is the probability that removing a cycle today will \emph{not} negatively impact the optimal matching size between $t$ and $t+h$. The higher this number, the more confident we are that clearing $c$ out today will not give us trouble in the future. A concrete example of this idea in shown in Figure \ref{fig:pseudo_reward_intuition}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/figures/pseudo_reward_intuition.pdf}
\caption{\textbf{Intuition for pseudo-rewards} \\
  In the example above, at time $t$ we have three available cycles to choose from: $(1,2)$, $(1,3)$ and $(3,4)$, and we are contemplating choosing cycle $(1,2)$ at $t$. If we  constrain ourselves to do so, several other future cycles become unavailable (in red). Nevertheless, in three out of our four simulations we were able to clear the same number of cycles in the constrained scenario, so the running pseudo-reward average is $\frac{3}{4}$. 
}
\label{fig:pseudo_reward_intuition}
\end{figure}

Pseudo-rewards are a natural way for us to control which patients should be matched today. Pairs that are easy to match will likely belong to many cycles, so by removing them we will be incurring a large cost in terms of future cycles that will become unavailable. But that means that the pseudo-reward associated with cycles that involve them will be lower, making them less attractive. On the other hand, patients that are harder to match will not participate many future exchange, so the price we pay for matching them today is low, and their average pseudo-reward is high. 



% ---------------------------------------------------------------------------
% Results
% ---------------------------------------------------------------------------

\section{Results}

\subsection{Direction prediction methods} 

Table \ref{tab:traditional_ml_classifier} shows the performance results for direct prediction methods as classification methods, i.e., how accurately they are able to predict whether a node should be matched or not. Results imply the following. 

First, while overall accuracy can be relatively high at 70-80\%, precision (defined as the ratio between true positives and both true and false positives) is low even for simpler environments like $ABO$, indicating that the models are over-predicting matchings. Second, augmenting the data with information about the graph has no discernible effect under any of the performance criteria. This suggests that there might be gains from using other algorithms that make better use of information about the compatibility graph. 

Unfortunately, the algorithms' substandard performance as classifiers translates into poor performance as policy as well. 



\subsection{Multi-armed bandit methods}





\section{Conclusion}

\paragraph{Summary}

\paragraph{Extensions}
Another source of error is the fact that we are predicting a label for each node independently from the labels we have predicted for other nodes.


 In reality, some exchanges are more or less desirable than others, either for ethical concerns or because of predicted health benefit. For example, it is common for pediatric patients and previous organ donors receive higher priority, as do exchanges involving patients with no HLA mismatch. 





\section{Conclusions}



% Appendix
\appendix
\section{Sections}

\section{Supplementary materials}


\begin{table}
  \centering
  \singlespacing
  \hspace*{-1cm}
  \input{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/tables/traditional_ml_classifier.tex}
   \caption{\textbf{Traditional methods} \\ Performance of traditional econometric and classification methods when predicting whether a node will be matched (1) or not (0). \emph{Additional} refers to extra information about graph structure passed to the classifier: \emph{Graph Stats} are graph-theoretical measures of node characteristics such as centrality; \emph{Embedding} is a 10-dimensional real embedding of the node provided by node2vec \citep{grover2016node2vec}. Recall is true positive rate $\frac{tp}{tp+fn}$; Specificity is true negative rate $\frac{tp}{tn+fp}$; Precision is $\frac{tp}{tp+fp}$; Accuracy is $\frac{tp+tn}{tp+tn+fp+fn}$. See section \ref{sec:[[[]]]} for methodology and section \ref{sec:[[[]]]} for discussion of results. }
  \label{tab:tradional_ml_classifier}
\end{table}

\begin{acks}
	
% The author would like to thank Utku \"{U}nver, Stefan Hoderlein, Arthur Lewbel, Mohammad Akbarpour, Itai Ashlagi, David Parkes, John Dickerson, Enkhmanlai Amarsaikhan and Sainbayar Sukhbaatar for helpful discussions.

The author would like to thank Itai Ashlagi for the idea of including a donor cPRA in the OPTN environment.

This work was supported in part by Health Resources and Services Administration contract 234-2005-37011C. The content is the responsibility of the authors alone and does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.

	
\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/references}

\end{document}
