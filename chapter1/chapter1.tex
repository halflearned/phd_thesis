\chapter*{Heterogenous Production Functions, Panel Data, and Productivity Dispersion}
\addcontentsline{toc}{chapter}{Heterogenous Production Functions, Panel Data, and Productivity Dispersion}
\begin{center}
\large
\textsc{Jeremy Fox, Vitor Hadad, Stefan Hoderlein} \\ \textsc{Amil Petrin, Robert Sherman}
\end{center}

\doublespacing

\section{Introduction}

% [One or two sentences providing basic introduction to the field, comprehensible to any economist]
Appropriately modelling unobserved heterogeneity is one of the foremost challenges faced by econometricians.
%[Two to three sentences of a more detailed background, comprehensible to econometricians]  %[One sentence clearly stating the general problem being addressed by this particular study]
In the context of panel data, it has become standard to controlling for unobserved heterogeneity via a \emph{fixed effects} approach, where we do not impose restrictions the distribution of latent heterogeneity parameters conditional on observable characteristics \citet{wooldridge2010econometric}. However, this flexibility often comes at a cost in terms of statistical modeling: the most ubiquitous fixed effects strategy in panel data is via the introduction of a time-invariant random intercept \citep{arellano2003panel}, but there are many instances of economic problems where this is might not be appropriate \citep{browning2007heterogeneity}.

%[One sentence summarizing main result]
Here, we present constructive identification and estimation results for the moments and marginal distribution of random coefficients in a linear panel data model with \emph{two} random coefficients
\begin{align}
  Y_{it} = A_{it} + X_{it}B_{it} + Z_{it}^{T}\beta_{t}
\end{align}

\noindent where the intercept $A_{it}$ and slope $B_{it}$ follow a specific autoregressive process but are otherwise allowed to be arbitrarily correlated with all regressors, and $Z_{it}$ is a (potentially large) vector of covariates associated with individual-invariant coefficients $\beta$.


% [Two or three sentences explaining what the main results reveals in direct comparison to what was thought to be case previously, or how the main result adds to previous knowledge]
% [One sentence to put the results into more general context]
Our results develop upon and extend the results of earlier work by \cite{graham2012identification}, who only focus on the first moments of the random coefficients and have stricter timing assumptions than ours. Another paper is \cite{arellano2011identifying}, who, like us, provide a method for identification and estimation of higher moments and distributional characteristics of random coefficients, but they maintain certain regularity assumptions that we do not impose.  Our model also permits that lagged dependent variables to be regressors. More broadly, this paper contributes to the larger literature of panel data models with unobserved heterogeneity, as we discuss below.


% [Two to three sentences to provide a broader perspective, readily comprehensible to a scientist in any discipline..]
In our particular application, we take $Y_{it}$ to be value-added log-production, while the two random coefficients represent total factor productivity (TFP) and the elasticity of one of the inputs in a Cobb-Douglas model of production. There is strong evidence for heterogeneity in this setting, where it is known as \emph{productivity dispersion}. In the United States, \cite{syverson2004product} finds large plant-level discrepancies in TFP, noting that within four-digit SIC industries ``the plant at the 90th percentile of the productivity distribution makes almost twice as much output with the same measured inputs as the 10th percentile plant", and a later survey by \cite{syverson2011determines} confirms this to be a typical finding. In fact, other work by \citep{hsieh2009misallocation} indicates that this gap may be small compared to other countries. In our work we note, however, that the large observed variation in TFP might be a product of restrictive statistical models: when only the intercept is able to vary, it may end up absorbing most the heterogeneity in the data, leading to inflated estimates of the standard deviation. On the other hand, we remark that our specific model and data differ from the ones above so we cannot directly compare our results, but leave it as a suggestion for future research.


% ----------------------
% LITERATURE REVIEW 
% ----------------------

\section{Background and literature review}

% The class of problems we're interested in
\paragraph{Panel data and heterogeneity} In full generality, panel data models can be represented as
\begin{align}
  Y_{it} = m(H_{it}, X_{it}, U_{it}) \label{eq:full_generality_panel}
\end{align}

\noindent where $Y_{it}$ is the dependent variable of interest for unit $i$ at period $t$, $H_{it}$ is an individual-specific, possibly infinite-dimensional object often called the \emph{unobserved heterogeneity} component, $X_{it}$ are observed characteristics, and $U_{it}$ is an idiosyncratic error term.  For example, $Y_{it}$ might represent an individual's wages over time, $X_{it}$ a vector of characteristics that includes educational attainment, $H_{it}$ a measure of their cognitive or manual ability, and $U_{it}$ measurement error. The econometrician is usually interested in some functional of this object, such as the average effect of a change in $X_{it}$ on $Y_{t}$ while keeping everything else fixed -- known as the \emph{average partial effect} (APE) \citet{blundell2003endogeneity}. 

% What is heterogeneity
Particular care must be taken with the \emph{unobserved heterogeneity} parameter $H_{it}$. \citet[~p.2]{browning2007heterogeneity} define \emph{heterogeneity} as ``the dispersion in factors that are relevant and known to individual agents when making a particular decision"\footnote{Browning \textit{et al} were, in turn, paraphrasing \cite{cunha2005separating}}. The qualifier \emph{unobserved} serves to emphasize that the econometrician does not have access to information about such factors. This is problematic because, even when $H_{it}$ itself is not of interest, it may preclude the identification of causal parameters. To continue our running example: even if we observe that workers with higher educational attainment receive higher wages, we cannot a priori claim that we have found a causal effect of $X_{it}$, as it may be that high-ability ($H_{it}$) workers are able to self-select into higher-paying jobs.

% Panel data models help with heterogeneity
One of the foremost advantages of dealing with panel data is that, under certain conditions, the econometrician may be able to leverage the existence of multiple observations per unit of analysis to control for this unobserved heterogeneity. This is possible, for example, if the researcher additionally imposes that $m$ is linear, and that all the unobserved heterogeneity can be explained by an individual but time-invariant shift, as in:
\begin{align}
  Y_{it} = A_{i} + X_{it}^{T}\beta + U_{it}
\end{align}

\noindent where all the heterogeneity $H_{it}$ is entirely subsumed in the random scalar intercept coefficient $A_{i}$. Such linear panel data models are the econometric ``workhorse in empirical studies" \citet{wooldridge2005fixed}.


% Fixed effects vs random effects
The average partial effect in this case is represented by components of the shared coefficient $\beta$, which can be identified under additional restrictions on the stochastic process governing $U_{it}$. In common econometric parlance, identification strategies that allow for $Cov(X_{it}, A_{it}) \neq 0$ are named \emph{fixed effects} models, while those that additionally impose that this covariance must be zero are called \emph{random effect} models.\citet{wooldridge2010econometric, arellano2003panel}.

% Intercept not enough
More generally, \cite{graham2012identification} define the class of \emph{fixed effects} methods as one that implies restrictions on the functional form of $m$ and on the conditional distribution of the idiosyncratic shocks $F(U_{it}|X_{\cdot}, A_{\cdot})$, but leaves the conditional distribution of unobserved heterogeneity $F(A_{it}|X_{it})$ unrestricted. A stronger class of \emph{random effects} methods additionally imposes an independence assumption between $A_{\cdot}$ and $X_{\cdot}$. While facilitating identification and estimation, the latter is harder to motivate economically -- it would mean, in our example, that workers' wages were unrelated to their unobserved ability (see \cite{griliches1977estimating} for a discussion).

\paragraph{Random coefficients} In the model above, all the heterogeneity is subsumed into one random scalar that entering additively. However, the ubiquitousness of such models may be explained more for their convenience than for their economic realism. As put by \citet[~p.11]{browning2007heterogeneity},

\begin{quote}
\singlespacing
Almost always decisions on how to include allowance for heterogeneity are made using conventional schemes that have been designed by statisticians to put in the heterogeneity in such a way that we can immediately take it out again. 
\end{quote}

% Therefore random coeffs
Therefore, in this paper we focus on partially rolling back some of the assumptions above and allow for a more general case with more than one correlated random coefficient. We will be working with a specific version of this general \emph{random coefficients} model

\begin{align}
  Y_{it} = A_{it} + X_{it}^T B_{it} + Z_{it}^T \beta
\end{align}

\noindent where now  $H_{it} = [A_{it}, B_{it}^T]^T$, so that now we have \emph{vector-valued heterogeneity}. Moreover, we will be principally interest with the case when $A_{it}, B_{it}$ are \emph{endogenous}, that is, not independent of $X_{it}$ (in the opposite case we say that the random coefficients are \emph{exogenous}).

The econometric literature dealing with this more general case is substantially smaller. Early work by \cite{swamy1970efficient} studied a panel data model with exogenous, stationary random coefficients and produced a feasible GLS estimator for random coefficient averages. \cite{mundlak1978models} and \cite{chamberlain1984panel} were one of the first to study panel data models with correlated random coefficients and derived efficient estimators under parametric assumptions about the distribution of random coefficients and errors. \cite{chamberlain1992efficiency} derived an efficient semiparametric estimator of the first moment of random coefficient under regularity assumptions. Later, \cite{arellano2011identifying} produced identification results for higher moments of the distribution of random coefficients under similar conditions, while \cite{graham2012identification} extended his approach to obtain the first moment in the \emph{irregular} case when such assumptions are not satisfied. As we will see, the main feature of irregular identification is convergence rates smaller than root-n \citep{lewbel2016identification}, which in turn happens due to the dependence on aspects of ``small" subpopulations defined on measure-zero sets. Our paper can be seen as a further development of these last two papers, as we identify all moments of the distribution of random coefficients under irregularity.

It should also be noted that even without panel data, there may be instances where aspects of random coefficients may be identified by if the variation in $X_{it}$ can be explained by variation in a set exogenous excluded variables. Examples of this \emph{triangular} model include \cite{heckman1998instrumental}, \cite{hoderlein2010analyzing}, and \cite{hoderlein2017triangular}.

\begin{figure}
  \includegraphics[width=1\textwidth]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/panel_data_models.pdf}
  \caption[Linear panel data models]{\textbf{Linear panel data models} \\ Different assumptions leading to different panel data models. Each different marker type and color indicates a different individual. \emph{Pooled}: all observations share same intercept and slope; \emph{Random effects}: each observation possesses their own time-invariant slope, but all share the same slope; \emph{Fixed effects}: same as random effects, but intercept and regressors can be correlated (figure shows positive correlation); \emph{FHHPS}: our paper generalizes the model by 1) allowing for intercept and slope to be correlated with regressors, and 2) allowing both intercept and slope to drift according to a specific Markovian process.}
  \label{fig:panel_data_models}
\end{figure}


\paragraph{Production function estimation} Our illustrative empirical application will involve the identification and estimation of a production function, so that $Y_{it}$ will represent output and $X_{it}$ inputs to production. This is a vast and comprehensive field on its own, so here we will simply note that two issues are pervasive throughout the literature and relevant to our paper.

First, there is a need to take heterogeneity into account when estimating production functions. The recognition of this fact goes at least as far back as \cite{marschak1944random} who note that [p. 145]
\begin{quote} 
\singlespacing
[T]he production function will change, even within the same industry, from firm to firm and from year to year, depending on the technical knowledge, the will, effort, and luck of a given entrepreneur: these factors (...) may be represented by one or more random parameters.
\end{quote}

Second, that this heterogeneity is plausibly endogenous, since a firm's choice of inputs is likely correlated with their own (unobserved) productivity \citep{gandhi2011identification}. Empirical applications usually address this issue via a dynamic panel approach (as in \cite{arellano1991some}) or via proxy variable approaches (as in \cite{levinsohn2003estimating}). 

In allowing for correlated random coefficients, our paper will incorporate a flexible form of vector-valued heterogeneity that takes the endogeneity problem into account.





\section{Model and assumptions}

From now on, in order to reduce clutter we will drop the $i$ subscript. For $1 \leq t \leq T$, define
\begin{align*}
Y_{t} &= A_{t} + B_{t}X_{t}
\end{align*}
where $X_{t}$ is a scalar, and $A_{t}, B_{t}$ are scalar random coefficients that evolve according to an AR(1) Markov process.
\begin{align}
  A_{t} &= A_{t-1} + U_{t} \\
  B_{t} &= B_{t-1} + V_{t}  
\end{align}

We will make the following sufficient assumptions for identifiability. Some of these are known to be stronger than necessary, and we will mention how to relax them later on. For ease of notation, let us denote $W_t = [X_t, X_{t-1}]^{T}$

\begin{assumption}{} \label{a:timeindependence}
  Conditional on recent covariates $W_t$, contemporaneous shocks are independent from past shocks.
  \begin{align}
    \text{For }2 \leq t \leq T ,
    \quad     
    (U_t, V_t) \perp \{ U_s, V_s \}_{s=0}^{t-1} \  | \ W_t
  \end{align}
\end{assumption}


\begin{assumption}{} \label{a:sequentialindependence}
  Shocks are sequentially independent from all covariates
  \begin{align}
    \forall t, \qquad U_t, V_t \perp (X_{1}, \cdots, X_{T})
  \end{align}
\end{assumption}


\begin{assumption}{}  \label{a:support}
  The covariate associated with the random coefficient is continuously distributed and is supported on the entire real line for all periods. Furthermore, their joint distribution is non-degenerate.
  \begin{align}
    \text{supp}(X_1) = \cdots = \text{supp}(X_T) = \mathbb{R} \qquad \text{and} \qquad \forall s \neq t, \ \ P(X_s = X_t) = 0 
  \end{align}
\end{assumption}

\begin{assumption}{}  \label{a:fullrank}
  Nonsingularity of the covariate matrix.
  \begin{align}
    \text{For }2 \leq t \leq T ,
    \quad     
    E[W_tW_t^{T} | X_1 = X_2] \qquad \text{is nonsingular}
  \end{align}
\end{assumption}


We should note that assumptions \ref{a:timeindependence} and \ref{a:sequentialindependence} allow for future regressors to be correlated with past shocks. In the context of firm profit maximization, this means that a firm is allowed to choose future inputs based on their previous productivity. In a different context, this model would also allow for a dynamic model with $X_t = Y_{t-1}$. Crucially, we do not make any assumptions of contemporaneous independence between random coefficients and regressors.

Assumption \ref{a:support} is a simplifying ``no-stayer" assumption that will facilitate some of the asymptotic derivations below. Assumption \ref{a:fullrank} is a common technical assumption often used for generic identification in linear models.

\section{Identifying random coefficient moments}

In this section, we show how to identify the $r^{th}$ moments of the random coefficients $(A_{t}, B_{t})$. Broadly, our identification strategy will be constructive, and follow these four steps.

\begin{enumerate}
  \item Identify shock moments $E[U_{t}^{\ell}V_{t}^{r-\ell}]$ 
  
  \item Express the conditional moments of the regressand $E[Y_{1}^\ell Y_{2}^{r - \ell} | W_{2i}]$ as a function of conditional moments of random coefficients $E[A_{t}^{\ell}B_{t}^{r-\ell}|W_{2}]$ and shocks $E[U_{t}^{\ell}V_{t}^{r-\ell}]$ 
  
  \item Invert the above so as to represent conditional moments of random coefficients as a function of conditional moments of the regressand and shocks $E[U_{t}^{\ell}V_{t}^{r-\ell}|W_{2}]$ 
  
  \item Integrate out to get unconditional moments $E[A_{t}^{\ell}B_{t}^{r-\ell}]$
\end{enumerate}



\subsection{First period moments}

Let us start with the first moment ($r = 1$). In this case, we are required to have $T \geq 2$ consecutive waves of panel data. 


We begin by simply reminding ourselves of the system of equations for these two periods.
\begin{align}
  \begin{cases}
    Y_{1} = A_{1} + B_{1}X_{1} \\ 
    Y_{2} = A_{2} + B_{2}X_{2} 
  \end{cases}
\end{align}

Let's expand the coefficients according to our model equations \ref{}.
\begin{align}
  \begin{cases}
    Y_{1} = A_{1} + B_{1}X_{1} \\ 
    Y_{2} = A_{1} + U_{2} + B_{1}X_{2} + V_{2}X_{t} \label{eq:expanded}
  \end{cases}
\end{align}


\paragraph{Step 1} In order to identify shocks, we consider the expectation of the difference between the two periods, conditioning on some point $W_2 = w_2 = (x, x)$.
\begin{align}
  E[Y_{2} - Y_{1} \ | \  W_{2} = w_{2}] = 
  E[U_{2}] + E[V_{2}]x
\end{align}

\noindent The expression above contains two caveats. First, that thanks our full support assumption \ref{a:support}, we can choose any point on $\mathbb{R}^2$ to condition on, including a point where $X_1 = X_2 = x$ as we just did. Second, that Assumption \ref{a:timeindependence} allowed us to drop the conditioning for the shocks.

Now, note that the moments $E[U_2]$ and $E[V_2]$ on the right-hand side are solutions to this minimization problem
\begin{align}
  \min_{\theta_u, \theta_v}  
  E[(Y_{2} - Y_{1} - \theta_u -  x\theta_v)^2 \  | \ W_2 = w_2]
\end{align}
\noindent and that the solution is unique, by virtue of the strict convexity of the quadratic function and the full rank assumption \ref{a:full_rank}. It follows that the shock moments $E[U_2]$ and $E[V_2]$ are generically identified, and so are $\beta_1$ and $\beta_2$.


\paragraph{Step 2} For this step, we simply have to take expectations of \ref{eq:expanded}, now conditioning on $W_2 = w_2 = (x_1, x_2)$. (Note we have dropped the constraint $X_1 = X_2$).
\begin{align}
  \begin{cases}
      E[Y_{1}|W_{2}]
        = E[A_{1}|W_{2}] + E[B_{1}|W_{2}]x_{1} \\ 
      E[Y_{2}|W_{2}]
        = E[A_{1}|W_{2}] + E[U_{2}] + E[B_{1} \ | \ W_{2}]x_{2} + E[V_{2}]x_{2} 
  \end{cases}
\end{align}

\paragraph{Step 3} Writing the previous equation in matrices after a little rearranging, we get:
\begin{align}
  \begin{bmatrix}
    E[Y_{1}|W_{2}] \\  
    E[Y_{2}|W_{2}] - E[U_{2}] -  E[V_{2}]x_{t} 
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 & x_1 \\
    1 & x_2  
  \end{bmatrix}
  \begin{bmatrix}
    E[A_{1}|W_{2}] \\
    E[B_{1}|W_{2}]
  \end{bmatrix}
\end{align}

\noindent As long as $x_1 \neq x_2$, we can invert the first matrix on the right-hand side. 
\begin{align}
  \begin{bmatrix}
    E[A_{1}|W_{2}] \\
    E[B_{1}|W_{2}]
  \end{bmatrix}
  =
  \begin{bmatrix}
    1 & x_1 \\
    1 & x_2  
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    E[Y_{1}|W_{2}] \\  
    E[Y_{2}|W_{2}] - E[U_{2}] -  E[V_{2}]x_{2}  \label{eq:inversion}
  \end{bmatrix}
\end{align}

\noindent since all the quantities on the right-hand side are known, we have identified all the conditional first moment of our random coefficients outside the line $X_1 = X_2$. Since our ``no-stayer" assumption \ref{a:support} guarantees that a randomly drawn point from the joint distribution of $X_1$ and $X_2$ will not be on the diagonal, we have generic identification.

Conditional second-period moments are also automatically identified, since due to our \ref{a:sequentialindependence} assumption we have that shocks are mean-independent from contemporaneous regressors, which allows us to write
\begin{align}
  E[A_2 | W_2 = w_2] = E[A_1|W_2 = w_2] + E[U_2] \\
  E[B_2 | W_2 = w_2] = E[B_1|W_2 = w_2] + E[V_2] \\
\end{align}

\paragraph{Step 4} The final step is simply to integrate out over the distribution of $W_2$ to get unconditional moments. 

\subsection{Second period moments} The identification procedure is analogous, but this time we use the square of our model equations.

\paragraph{Step 1} Identify shock second moments on the diagonal $W_2 = w_2 = (x, x)$ using the following equation
\begin{align}
  E[(Y_{2} - Y_{1})^2 \ | \  W_{2} = w_{2}] = 
  E[U_{2}^2] + E[V_{2}^2]x^2 + 2E[U_{2}V_{2}]x
\end{align}

\noindent The analogous minimization procedure is
\begin{align}
E[U_2^2], E[V_2^2], E[U_2V_2] =
  \min_{\theta_{uu}, \theta_{vv}, \theta_{uv}}  
  E[(Y_{2} - Y_{1} - \theta_{uu} -  x^2\theta_{vv} - 2x\theta_{uv})^2 \  | \ W_2 = w_2]
\end{align}

\noindent point identification is once more possible because the solution to this minimization problem is unique.

\paragraph{Steps 2-3} Again drop the constraint $X_1 = X_2$, and conditioning on $W_2 = w_2 = (x_1, x_2)$ consider the expectation of the square of our model equations.

\begin{align}
\begin{bmatrix}
  E[A_1^2 | x_{1}, x_{2}] \\ 
  E[B_1^2 | x_{1}, x_{2}]  \\ 
  E[A_1 B_1 | x_{1}, x_{2}]
\end{bmatrix}
=
\begin{bmatrix}
  1 & x_{1}^2 & 2x_{1} \\ 
  1 & x_{2}^2 & 2x_{2} \\ 
  1 & x_{1}x_{2} & x_{1} + x_{2} \\ 
\end{bmatrix}^{-1}
\begin{bmatrix}
  E[Y_1^2 | x_{1}, x_{2}] \\
  E[Y_2^2 | x_{1}, x_{2}] - C_{1i} \\
  E[Y_1 Y_2 | x_{1}, x_{2}] - C_{2i} \\
\end{bmatrix}
\end{align}

\noindent where $C_{1i}$ and $C_{2i}$ involve only quantities that were already estimated in previous steps.
\begin{align}
C_{1i} &= E[U^2] + 2E[U_2 V_2] x_{2} +  E[V_2^2]x_{2}^2 \\
      &- 2\{
E[A_1 | x_{1}, x_{2}] +
E[B_1 | x_{1}, x_{2}]x_{2}
\}
\{
E[U_2] + E[V_2]x_{2}
\}
\\
C_{2i} &= 
\{ 
  E[A_1 | x_{1}, x_{2}] + E[B_1 | x_{1}, x_{2}]x_{1}
\}
\{
E[U_2] + E[V_2]x_{2}
\}
\end{align}

Note that the relevant matrix above is invertible whenever $x_1 \neq x_2$. 

\paragraph{Step 4} Analogous to step 4 in the first moment case.


\subsection{Identifying further moments}

The argument above generalizes for arbitrary moments. In fact, if we are only after moments a finite number of moments of the distribution, then assumption \ref{a:sequentialindependence} as follows. Let $r$ be a positive integer and for $t \geq 2$ define $W_t = (X_{t-1}, X_{t})$. We will say that $(U_t, V_t)$ are \emph{r\textsuperscript{th}-moment independent} of $W_t$ if 
  \begin{align}
    \text{For }1 \leq j \leq r,
    \quad     
    E[U_{t}^{r-j} V_{t}^{j} | W_t] = E[U_{t}^{r-j} V_{t}^{j}]
  \end{align}
\end{definition}

We can replace assumption \ref{a:sequentialindependence} by the following weaker assumption.

\paragraph{Assumption 3.1'} \label{a:sequentialindependence}
  \textit{Shocks are r\textsuperscript{th}-moment independent from all covariates}
  \begin{align}
    \text{For }1 \leq j \leq r,
    \quad     
    E[U_{t}^{r-j} V_{t}^{j} | W_t] = E[U_{t}^{r-j} V_{t}^{j}]
  \end{align}
  
  

\subsection{Extension: fixed coefficients}

Our model can be augmented with an arbitrary number of regressors associated with coefficients that are individual-invariant -- but not necessarily time-invariant. The model equations become
\begin{align*}
Y_{t} &= A_{t} + B_{t}X_{t} + Z_{t}^T \beta_{t} \qquad \beta_{t}: \text{fixed}
\end{align*}

Identification of the fixed coefficients happens on Step 1, when the minimization problem becomes
\begin{align}
 E[U_2], E[V_2], \beta_1, \beta_2  = \min_{\theta_u, \theta_v, \theta_{b_1}, \theta_{b_2}}  
  E[(Y_{2} - Y_{1} - a - bx_2 + z_2^{T}\theta_{b_1} - z_1^{T}\theta_{b_2})^2 | W_2 = w_2]
\end{align}
and the argument follows for the same reasons that were previously explained. Once we have identified $\beta_1, \beta_2$, we remove them from future computations using the modified regressand $\tilde{Y_t} := Y_{t} - Z_{t}\beta_{t}$, and the rest of the process delineated above goes through without further change.



\subsection{Identifying the marginal distribution of random coefficients}

In this section we will prove that we can identify the marginal density of random coefficients $f_{A_tB_t}$. We begin by strengthening some of the assumption above.

\paragraph{Assumption 1.1'} \label{a:sequentialindependence}
\textit{Shocks are jointly independent of past shocks and current and past regressors}
\begin{align}
  \text{For }1 \leq j \leq r,
  \quad     
  (U_{t}, V_{t}) \perp (U_{1}, V_{1}, \cdots, U_{t-1}, V_{t-1}, X_{1}, \cdots, X_{t-1})
\end{align}

\paragraph{Assumption 1.5} \label{a:}
\textit{The joint distribution of shocks is positive everywhere}
\begin{align}
  \text{For }1 \leq j \leq r,
  \quad     
  \phi_{U_t, V_t}(s_1, s_2) := E[\exp(i[s_1 U_{t_1} + s_{2} V_{t_2}])] > 0 \qquad \forall t \in \mathbb{R}^2
\end{align}  

\begin{theorem}[Nonparametric identification] If Assumptions 1.1', 1.3, 1.4 and 1.5 are satisfied, then the joint distribution of random coefficients $f_{A_t, B_t}$ is identified for all $t$.
\end{theorem}

\paragraph{Proof sketch} Consider two adjacent periods, here denoted by $t = 1,2$. Let $\phi_{Y_{1}, Y_{2}}(s_1, s_2| x_{1},x_{2}) := E[\exp(i[s_1 Y_{1} + s_{2} Y_{2}])| X_{1} = x_{1}, X_{2} = x_{2}]$ be the joint characteristic function of the dependent variable conditional on observable covariates. We can identify the characteristic function of shocks by evaluating this characteristic function at the ``diagonal" $X_1 = X_2 = x$ with $s_{2} = -s_{1} = s$.
\begin{align}
\phi_{Y_{1}, Y_{2}}(-s, s| x, x) &= E[\exp(i[sY_{2} - sY_{1}])| X_{1} = X_{2} = x] \\
                          &= E[\exp(i[sU_{2} - sxV_{2}])| X_{1} = X_{2} = x] \\ 
                          &= E[\exp(i[sU_{2} - sxV_{2}])] \qquad \because \text{Assumption 1.1'}    \\
                          &=: \phi_{U_{2},V_{2}}(s, s') \qquad s' := sx          
\end{align}

Applying the inverse Fourier transform to the joint characteristic function of the shocks $\phi_{U_{2},V_{2}}$, we retrieve their distribution $f_{U_{2}, V_{2}}$. Note that the full support Assumption 1.3 was used in the inversion step.

Next, as in the moment identification discussion, write
\begin{align}
  \begin{bmatrix} Y_{1} \\ Y_{2} \end{bmatrix}
  =
  \Gamma_1(x_1, x_2) \begin{bmatrix} A_{1} \\ B_{1} \end{bmatrix} +
  \Gamma_2(x_1, x_2) \begin{bmatrix} U_{2} \\ V_{2} \end{bmatrix}  
\end{align}
\noindent for appropriate matrices $\Gamma_1$ and $\Gamma_2$. Now choose $t_1 = s^{T}\Gamma_{1}$ and $t_2 = t\Gamma_{1}^{-1}(x_1, x_2)\Gamma_2(x_1, x_2)$. Note that this exists whenever $x_1 \neq x_2$. The conditional characteristic function of $Y_{1}, Y_{2}$ then decomposes multiplicatively:
\begin{align}
\phi_{Y_{1}, Y_{2}}(t_1, t_2| x_1, x_2) &= E[\exp(it_1^{T}Y_{1} + it_2^{T}Y_{2}])  | X_{1} = x_1, X_{2} = x_2] \\
          &= E[\exp(is\cdot [A_1, B_1] - it_2 \cdot [U_2, V_2] ) | X_{1} = x_1, X_{2} = x_2] \\    
          &= E[\exp(is\cdot [A_1, B_1] | X_{1} = x_1, X_{2} = x_2]\cdot E[\exp(it_2 \cdot [U_2, V_2] )] \\
          &=: \phi_{A_1, B_1}(s_1, s_2 |x_1, x_2)\cdot  \phi_{U_2, V_2}(t_2) \label{eq:decomposition}
\end{align}

Since we have already identified $\phi_{U_2, V_2}(s)$ for all $s \in \mathbb{R}^2$, we can retrieve $\phi_{Y_1, Y_2}(t_2|x_1, x_2)$ by dividing both sides of Equation \ref{eq:decomposition} by $\phi_{U_2, V_2}(t_2)$ whenever $t_2$ exists. This implies that we can retrieve the conditional density function $f{A_1, B_1}(a_1, b_1 | x_1, x_2)$ by applying the inverse Fourier transform to the ratio $\frac{\phi_{Y_1, Y_2}(t_2|x_1, x_2)}{\phi_{U_2, V_2}(t_2)}$. Finally, averaging over $X_1, X_2$, we can retrieve the marginal density $f{A_1, B_1}$.\footnote{We are glossing over the fact that we have not identified the characteristic function over the diagonal $x_1 = x_2$. For the complete argument please see a supplementary note.}


%--------------
% ESTIMATION
%------------- 


\section{Estimation} \label{sec:estimation}

Our estimation procedure follows three steps:

\begin{enumerate}
  \item Use local polynomial regression to estimate shock moments and individual-invariant coefficients
  \item Use any nonparametric estimator to retrieve estimates of random coefficient moments conditional on observables
  \item Average out conditional random moments by averaging out conditional moments, taking care to avoid
  observations for which $X_1 \approx X_2$.
\end{enumerate}

Let us consider each one of these steps in detail.

\subsection*{Estimating shocks}

\paragraph{First moments} Regress $\Delta Y_2$ on $X_2$ locally at the diagonal $X_1 \approx X_2$:
\begin{align}
  \hspace*{-1cm}
  (\widehat{E}[U_2], \widehat{E}[V_2], \ \cdot \ ) = \arg \min_{\theta_{U} ,\theta_{V}, \gamma}
\sum_{i} K_h (\Delta X_{2i}) \cdot 
      \left( \Delta Y_{2i} - \theta_{U} - X_{2i}\theta_{V} - g_1(X_{2i}, \Delta X_{2i}; \gamma \right)^2
\end{align}

\noindent where $K_h(\cdot)$ is a standard kernel with asymptotically zero bandwidth:
\begin{align}
  h_{shocks}(n) = c_{shocks}n^{−\alpha_{shocks}} \qquad \text{where } \lim_{n\rightarrow \infty} h_{n} = 0
\end{align}
\noindent and $g_1$ is a $K$-order polynomial in $X_2$ and $\Delta X_2$ with coefficients $\gamma$. In theory its inclusion is optional, but we have observed that its presence substantially improves the quality of the estimates. 

If our model included time-invariant coefficients, they would also be estimated in this step.



\paragraph{Second moments} We estimate the uncentered moments by proceeding similarly to above:
\begin{align}
  (\widehat{E}[U_2^2], \widehat{E}[U_2 V_2] &\widehat{E}[V_2^2], \ \cdot \ ) = \\  
  \arg \min_{\theta_{U_2} , \theta_{UV}, \theta_{V_2}, \gamma}
\sum_{i} K_h(\Delta X_{2i}) &\cdot 
      \left( \Delta Y_{2i}^2 - \theta_{U_2} - 2X_{2i}\theta_{UV}  - X_{2i}^2\theta_{V_2} - g_2(X_{2i}, \Delta X_{2i}; \gamma) \right)^2
\end{align}

Naturally, once we have uncentered second moments and first moments, we can compute estimates of centered moments using the usual formulas for centered moments, e.g. $\widehat{Var}[U_2] = \widehat{E}[U_2^2] - \widehat{E}[U_2]^2$. 

\subsection*{Conditional random coefficient moments}



\paragraph{Conditional moments of $Y_1$ and $Y_2$} Here we begin by estimating $E[Y_1^{m} Y_2^{n}|X_1 = x_1, X_2 = x_2]$ for $m,n \in \{ (1,0), (0,1), (1,1), (2,0), (0,2)\}$. This can be done using any nonparametric estimator, but we choose the Nadaraya-Watson for simplicity.
\begin{align}
  \widehat{E}[Y_1|X_1 = X_1, X_2 = X_2] =
  \frac{\sum_i Y_{1i}K_h(X_{1i} - x_1) K_h(X_{2i} - x_2)}
  {\sum_i K_h(X_{1i} - x_1) K_h(X_{2i} - x_2)}
\end{align}

\noindent where $K_h$ is again a standard kernel endowed with asymptotically vanishing bandwidth.
\begin{align}
  h_{nw}(n) = c_{nw}\hat{\sigma} n^{-\alpha_{nw}}
\end{align}

\noindent where $\hat{\sigma}$ is an estimate of $Std(X_1) = Std(X_2)$. Note that for the theoretical MISE-minimizing bandwidth, one should set $\alpha_{nw} = \frac{1}{6}$. 

\paragraph{Solving for first moments} Once in possession of all the previous estimates, we can solve for estimates of conditional random coefficient moments using the empirical analogs of equations \ref{eq:first_moment_inversion} and \ref{eq:second_moment_inversion}.
  

\subsection*{Unconditional random coefficient moments}

In this final step, we must exclude observations near the diagonal $X_1 ≈ X_2$, and then average the conditional moments associated with the remaining observations. For the first moment of the intercept, we compute 

\begin{align}
  \hat{E}[A_1] = \frac{\sum_{i=1}^{n} \hat{E}[A_1] | X_{1i}, X_{2i} \cdot 1\{ |X_{2i} - X_{2i}| > t_{1}(n) \} }
                      {\sum_{i=1}^{n} 1\{ |X_{2i} - X_{2i}| > t_1(n) \} }
\end{align}

\noindent and analogously for the slope and for higher moments. Here, $t_{1}(n)$ and $t_2(n)$ (for second moments) are \emph{censoring threshold} functions that vanish asymptotically. Consistency and asymptotic normality of the resulting estimates are ensured provided that
\begin{align}
 t_1(n) \equiv c_{cens1} \hat{\sigma}_{X_2} n^{-\frac{1}{4}} \quad \text{ for first moments}\\
 t_2(n) \equiv c_{cens1} \hat{\sigma}_{X_2} n^{-\frac{1}{4}} \quad \text{ for second moments}
\end{align}

\noindent where $c_{cens1}$, $c_{cens2}$ are constants that are not pinned down by theory. In order to choose these constants, we resort to experimentation as shown in section \ref{sec:simulation}.


%-------------------
% Asymptotic results
%--------------------

\section{Asymptotic results}

In this section we present an abridged version of our asymptotic results without proof.\footnote{For an elaborated discussion, please see supplementary material available upon request.)}  

\begin{definition} Define the following objective functions
  \begin{align}
    \hat{G}_{n}(\alpha) &= \frac{1}{n} \sum_{i=1}^{n} \left [\left( \hat{E}(A_1|X_1, X_2) - \alpha \right)^2 1\{ | X_2 - X1 | > h_n \} \right] \label{eq:hat_g} \\ 
    G_{n}(\alpha) &= E\left [\left( \hat{E}(A_1|X_1, X_2) - \alpha \right)^2 1\{ | X_2 - X1 | > h_n \} \right]  \\ 
    \Gamma(\alpha) &= E\left [\left( \hat{E}(A_1|X_1, X_2) - \alpha \right)^2 \right]  \label{eq:gamma}
  \end{align}
  and let $\hat{\alpha}$ and $\alpha_{0}$ be the minimizers of \ref{eq:hat_g} and \ref{eq:gamma}. Explicitly:
  \begin{align}
    \hat{\alpha} &= \frac{ \sum_{i=1}^{n} \hat{E}[A_{1} | X_{i1}, X_{2i}] 1\{| X_{2i} - X_{1i}| > h_{n}\}  } { \sum_{i=1}^{n} 1\{ |X_{2i} - X_{1i}| > h_{n} \} } \\
    \alpha_{0} &= E[E[(A_1 | X_{1}, X_{2})]] = E[A_{1}]
  \end{align}
\end{definition}



\begin{theorem}[Consistency] If the following two conditions hold 
  \begin{enumerate}
    \item $\sup_{\alpha} \left| \hat{G}_{n}(\alpha) - \Gamma(\alpha) \right| \in o_{p}(1)$
      \item For each $\delta > 0$, $\inf_{\alpha: |\alpha - \alpha_{0}| \Gamma(\alpha) > \Gamma(\alpha_0)}$ 
  \end{enumerate}
  Then $|\hat{\alpha} - \alpha_{0}| \in o_{p}(1)$
\end{theorem}


\begin{theorem}[Asymptotic Normality for conditional moments] Suppose that $x_1 \neq x_2$ and $f(x_1, x_2) > 0$. If the following conditions hold
  \begin{enumerate}
    \item For $k \in \{0, 1\}$, the function $r_{k}(x_1, x_2, \theta)$ defined below has continuous second partial derivative with respect to $x_1$.
    \begin{align}
      r_{k}(x_1, x_2, \theta) = E[(Y_2  - Y_1 - EU_2 - EV_2X_2)X_2^{k} | X_1 = x_1, X_2 = x_2]f(X_1 = x=1, X_2 = x_2)
    \end{align}
    \item The function $s_t(x_1, x_2) = E[Y_1 | X_1 = x_1, X_2 = x_2]f(X_2 = x_2 | X_1 = x_1)$ is twice continuously differentiable
    \item The marginal density of the first regressor $f(x_1)$ is twice continuously differentiable
    \item The conditional means of $Y_{1}$ and $Y_{2}$ are estimated with uniform kernels endowed with bandwidth $h_{nw}(n) \propto n^{-\alpha_{nw}}$, where $\frac{1}{5} < \alpha_{nw} < \frac{1}{2}$
    \item The censoring threshold functions decay as the following rates $t_1(n) \propto n^{-\frac{1}{4}}$ and $t_2 \propto n^{-\frac{1}{8}}$
  \end{enumerate}
\noindent 
Then the estimates of conditional first and second moments are asymptotically normal.
% \begin{align}
%   \sqrt{nh_n^2}
%   \begin{bmatrix}
%     \widehat{E}[A_{1} | X_1 = x_1, X_2 = x_2] - E[A_1 | X_1 = x_1, X_2 = x_2] \\
%     \widehat{E}[A_{1} | X_1 = x_1, X_2 = x_2] - E[B_1 | X_1 = x_1, X_2 = x_2] 
%   \end{bmatrix}
%   \rightarrow^{D}
%   \mathcal{N}
%     \left(
%     \begin{bmatrix} 0 \\ \end{bmatrix},
%     V(x_1, x_2)
%     \right)
% \end{align}
% \noindent for a given matrix $V(x_1, x_2)$. A similar result holds for 
\end{theorem}



% -------------
%  Simulations
% -------------


\section{Simulations} \label{sec:simulation}

The purpose of this sections is to provide a simulation study in a setting that is similar to our empirical application. We begin by generating the first period random coefficients $(A_1, B_1)$, their second-period shocks $(U_2, V_2)$,
and regressors for both periods $(X_1, X_2)$. They are drawn from a jointly Normal distribution:

\begin{align} \label{eq:simulation_model}
\begin{bmatrix}
  A_1\\
  B_1\\
  X_1\\
  X_2\\
  U_2\\
  V_2 
\end{bmatrix}
\sim
\mathcal{N}\left(
\begin{bmatrix}
2 \\ 
1 \\ 
0 \\ 
0 \\ 
.3 \\
.1 
\end{bmatrix}
,
\begin{bmatrix}
  9 & 0.95 & 1.5 & 1.5 & 0 & 0 \\
  0.95 & 0.4 & 0.32 & 0.32 & 0 & 0 \\
  1.5 & 0.32 & 1 & 0.5 & 0 & 0 \\
  1.5 & 0.32 & 0.50 & 1 & 0 & 0 \\ 
  0 & 0 & 0 & 0 & 1 & 0.16 \\
  0 & 0 & 0 & 0 & 0.16 & 0.1 
\end{bmatrix}
\right)
\end{align}

These number were chosen to roughly reflect the characteristics that we expect them to have in a real data
application. The implied correlation matrix has a very simple structure.

\begin{align}  \label{eq:simulation_correlation}
  \begin{bmatrix}
      1&  0.5& 0.5&  0.5&  0&  0 \\
      0.5&  1& 0.5&  0.5&  0&  0 \\
      0.5& 0.5&  1&  0.5&  0&  0 \\
      0.5& 0.5& 0.5&   1&  0&  0 \\
      0&   0&  0&   0&  1& 0.5 \\
      0&   0&  0&   0& 0.5&  1
  \end{bmatrix}  
\end{align}

The remaining variables were created from these in the obvious manner. In order to understand how our estimator behaves as the number of observation increases, we used $n \in \{500, 2000, 5000, 10000, 20000 \}$.

As seen in Section \ref{sec:estimation}, our method requires the careful tuning of many different parameters. Since our methods are not amenable to cross-validation, we simulated the model in section 1 over a large grid of parameter combinations, which were then compared according to their RMSE against the true value. The RMSE-minimizing parameter configuration for each number of observations is shown on Table \ref{tab:best_parameters}. 

\begin{table}[htdp]
  \centering
  \renewcommand{\arraystretch}{1.6}
  \begin{tabular}{l|cccc}
    n & $c_{shocks}$ & $\alpha_{shocks}$ & $c_{nw}$ & $\alpha_{nw}$ \\  \hline
    500 & 2 & $\frac{1}{5}$ & $\frac{1}{2}$ & $\frac{1}{6}$ \\
    2000 & 3 & $\frac{1}{5}$ & $\frac{1}{2}$ & $\frac{1}{6}$ \\
    5000 & 4 & $\frac{1}{5}$ & $\frac{1}{2}$ & $\frac{1}{6}$ \\
    10000 & 4 & $\frac{1}{5}$ & $\frac{1}{2}$ & $\frac{1}{6}$ \\
    20000 & 3 & $\frac{1}{5}$ & $\frac{1}{2}$ & $\frac{1}{3}$ \\
  \end{tabular}
  \caption{RMSE-minimizing parameter configurations for our DGP.}
  \label{tab:best_parameters}
\end{table}
 
Note that the best parameters remain fairly stable as we increase the number of observations. The most notable variation is in $\alpha_{nw}$: intuitively, when the number observations is large enough, a much smaller bandwidth is preferred, and that is translated into a more aggressive choice of exponent. We will use these numbers to guide our choice of tuning parameters in our empirical application in the next section.

The performance of our estimators is illustrated on Figures \ref{fig:simulation_shocks} and \ref{fig:simulation_random_coefficients} and on the accompanying tables on the Appendix. We observe that indeed the distribution tends to concentrate around its true value as the number of observations increases. 


\subsection{Bootstrap coverage}

Tables \ref{tab:bs_coverage_shocks}-\ref{tab:bs_coverage_random_coefficients} show the coverage of bootstrap confidence intervals for all estimated parameters. To produce these tables, we generated 2000 datasets using different seeds, computed 500 bootstrap estimates in each of these datasets, and calculated their 0.025 and 0.975 quantiles. The numbers in each cell are the proportion of times that quantiles straddled the true value.

\begin{table}[H]
  \singlespacing
  \caption{Bootstrap coverage for shock moments}
  \input{chapter1/tables/bs_coverage_shocks.tex}
  \label{tab:bs_coverage_shocks}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Bootstrap coverage for random coefficient conditional moments}
  \input{chapter1/tables/bs_conditional_coverage.tex}
  \label{tab:bs_conditional_coverage}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Bootstrap coverage for random coefficient unconditional moments}
  \input{chapter1/tables/bs_coverage_random_coefficients.tex}
  \label{tab:bs_coverage_random_coefficients}
\end{table}

\section{Empirical application}

Now we provide an illustration of our methods by estimating a Cobb-Douglas production function with random coefficients for a large number of Indian plants. We will be using the Annual Survey of Industries (ASI) dataset, released by the Central Statistical Organization of India for the years 2008 and 2009. This yearly survey collects data from sampled Indian economic units of production (individual factories, workshops, and establishments, hereafter called ``plants") that employ more than 10 regular workers. 

From the ASI dataset we extract four variables: \emph{gross sale value} (value of the products sold by the plant, as purchased by their clients) $S_{t}$; \emph{capital} (fixed assets with a productive life of more than one year) $K_{t}$; \emph{wages} $W_{t}$; and production \emph{materials} $M_{t}$. All variables are in 2005 rupees. We keep only observations that we present in both years of analysis, and we are left with 13298 observations.\footnote{Additional information about data cleaning and manipulation is available as Jupyter Notebook at \url{https://github.com/halflearned/FHHPS/blob/master/empirical/FHHPS_data_cleaning.html}}


In addition to the variables above, we generate our model variables:
\begin{align}
  Y_{t} = \log\left( \frac{S_{t} - M_{t}}{W_{t}}\right) \qquad X_{t} = \log \left( \frac{K_{t}}{W_{t}} \right)
\end{align}
\noindent where $Y_{t}$ represents production value-added after sales normalized by wages, and $X_t$ represents normalized capital. Summary statistics for all variance can be found in subsection \ref{sec:summary_statistics} in the Appendix. 

\paragraph{Interpretation} Each plant's output is modeled as a Hicks-neutral Cobb-Douglas function. Therefore, the random variable $A_{t}$ represents \emph{total factor productivity}, while $B_{t}$ represents the expenditure share on capital inputs.å

Included in the modeling decision above there are several assumptions. First, timing: we are assuming that each firm observes the entire history of past inputs and outputs $(Y_{1}, X_{1}, \cdots, Y_{t-1}, X_{t-1})$, as well as the values of their own TFP and capital share $(A_{1}, B_{1}, \cdots, A_{t-1}, B_{t-1})$. Based on this information, they choose this period's inputs $X_{t}$. Once the decision is taken, this period's shocks $U_{t}, V_{t}$ are revealed, and production $Y_{t}$ is realized.

Second, we have left the stochastic process governing the evolution of $X_{t}$ unrestricted. In particular, we do not impose the assumption that firms are profit-maximizers -- although they may be. This is important for studying unproductive firms as well.



\begin{table}[H]
  \singlespacing
  \caption{Empirical application: Shock Estimates}
  \input{chapter1/tables/bootstrap_shock_empirical.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Empirical application: Random Coefficient Estimates (first period)}
  \input{chapter1/tables/bootstrap_random_coefficients_1_empirical.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Empirical application: Random Coefficient Estimates (second period)}
  \input{chapter1/tables/bootstrap_random_coefficients_2_empirical.tex}
\end{table}




\begin{figure}
  \centering
  \includegraphics[height=0.8\textheight]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/simulation_shocks.pdf}
  \label{fig:simulation_shocks}
  \caption[Shock estimates in simulated data]{Shock estimates in simulated data}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=0.8\textheight]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/simulation_random_coefficients.pdf}
  \label{fig:simulation_random_coefficients}
  \caption[Random coefficient estimates in simulated data]{Random coefficient estimates in simulated data}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=0.8\textheight]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/shocks_empirical.pdf}
  \caption[Shock estimates using ASI data]{Shock estimates using ASI data}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[height=0.8\textheight]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/rc1_empirical.pdf}
  \caption[Random coefficient estimates using ASI data (2008)]{Random coefficient estimates using ASI data (2008)}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=0.8\textheight]{/Users/vitorhadad/Documents/kidney/matching/phd_thesis/chapter1/figures/rc2_empirical.pdf}
    \caption[Random coefficient estimates using ASI data (2009)]{Random coefficient estimates using ASI data (2009)}
\end{figure}





\clearpage


\section{Conclusion and future work}


What if T > 2? The approach generalizes but seems to become more complicated (as for example U2,V2 are correlated with X3,X4,...).
• It would be useful to see how strict the timing assumptions are (in standard linear panel models different timing assumptions can be dealt with by changing the set of instruments). For example: what if U2 independent of X1 but not of X2,X3,...?
• The authors’ setup could be used to learn about the firms’ ob- jectives, by estimating the determinants of input choice: Xit given
Ai,t−1, Bi,t−1, Xi,t−1, ....
• Looking ahead: nonlinear models? Example: CES.


% ------------
% APPENDIX
% ----------

\section{Appendix}

\subsection{Summary statistics} \label{sec:summary_statistics}



\begin{table}[H]
  \singlespacing
  \caption{ASI survey data: summary statistics}
  \input{chapter1/tables/summary_statistics_raw1.tex}
  \label{tab:summary_statistics_raw1}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{ASI survey data: correlations}
  \input{chapter1/tables/summary_statistics_raw2.tex}
  \label{tab:summary_statistics_raw2}
\end{table}


\begin{table}[H]
  \singlespacing
  \caption{Transformed variables: summary statistics}
  \input{chapter1/tables/summary_statistics_processed1.tex}
  \label{tab:summary_statistics_processed1}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Transformed variables: correlations}
  \input{chapter1/tables/summary_statistics_processed2.tex}
  \label{tab:summary_statistics_processed2}
\end{table}


\subsection{Simulation results}

Here we show how our estimates of shock and random coefficient moments change as we increase the number of observations. Throughout, we used the RMSE-minimizing parameters shown in table \ref{tab:best_parameters}. To produce the tables below, we generated 8000 datasets for each number different number of observations, produced the relevant estimates and computed the performance measures.

\subsubsection*{Shock moments}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of E[U_2]}
  \input{chapter1/tables/simulation_statistics_EU.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of E[V_2]}
  \input{chapter1/tables/simulation_statistics_EV.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Std[U_2]}
  \input{chapter1/tables/simulation_statistics_SU.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Std[B_2]}
  \input{chapter1/tables/simulation_statistics_SB.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Cov[U_2, V_2]}
  \input{chapter1/tables/simulation_statistics_CUV.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Corr[U_2, V_2]}
  \input{chapter1/tables/simulation_statistics_CorrUV.tex}
\end{table}


\subsection*{Random coefficient moments}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of E[A_1]}
  \input{chapter1/tables/simulation_statistics_EA.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of E[B_1]}
  \input{chapter1/tables/simulation_statistics_EB.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Std[A_1]}
  \input{chapter1/tables/simulation_statistics_SA.tex}
\end{table}
 
\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Std[B_1]}
  \input{chapter1/tables/simulation_statistics_SB.tex}
\end{table}

\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Cov[A_1, B_1]}
  \input{chapter1/tables/simulation_statistics_CAB.tex}
\end{table}
 
\begin{table}[H]
  \singlespacing
  \caption{Performance measures for estimates of Corr[A_1, B_1]}
  \input{chapter1/tables/simulation_statistics_CorrAB.tex}
\end{table}

 
\subsection{Python module \texttt{FHHPS}}

In order to facilitate the adoption of the methods described in this paper by other researchers, I have developed the Python module \texttt{FHHPS}. Instructions for installations and usage can be found in the github repository\footnote{Url: https://github.com/halflearned/FHHPS}. Its API is inspired by the celebrated \texttt{scikit-learn} library\cite{sklearn_api}, and it should be familiar to statisticians and machine learning practitioners that use Python. The webpage linked above also contains one-line instructions for reproducing all of our figures and tables. 

\clearpage
\begin{landscape}
\begin{table}[H]
  \singlespacing
  \caption{Panel regressions}
  \input{chapter1/tables/panel_lm_table.tex}
  \label{tab:panel_lm_table}
\end{table}
\end{landscape}


 


